\section{Architecture Design}
\label{sec:arch}

\tresumo{Arquitetura e Processo: Apresentação da topologia e do fluxo automatizado de deploy do cluster (Terraform -> Cloud-Init -> Ansible).}

\bnote{ATENÇÃO: As referências para as figuras fig:prov-terraform, fig:prov-os e fig:prov-cluster constam no texto mas as imagens não estão no arquivo latex inseridas com \texttt{\textbackslash begin\{figure\}}.}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{content/assets/topology.png}
    \caption{Topology of Cloud Config on OCI.}
     \label{fig:topology}
\end{figure}

Figure~\ref{fig:prov-terraform} depicts the Terraform execution
perspective of the provisioning workflow. It covers RSA key-pair
generation, network resource creation (VCN, Internet Gateway, DRG,
Route Table, Security List), IPSec VPN configuration, compute
instance provisioning with cloud-init \texttt{user\_data}, and
the upload of Ansible playbooks and cluster assets to the master
node via SSH file provisioner.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{content/assets/provisioning-01-terraform.png}
  \caption{Provisioning workflow — View 1: Terraform execution sequence.}
  \label{fig:prov-terraform}
\end{figure}

Figure~\ref{fig:prov-os} details the OS-level configuration phase,
triggered by cloud-init on all VMs immediately after boot. It
covers the concurrent execution on the master (Python/Ansible
installation, SSH key injection, Ansible inventory setup) and
workers (SELinux and firewall disabling, chrony), followed by
the full \texttt{site.yml} Ansible playbook: base package
installation, PostgreSQL initialisation, Ambari Server setup,
ODP repository configuration, and Ambari Agent deployment across
all cluster nodes.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{content/assets/provisioning-02-os.png}
  \caption{Provisioning workflow — View 2: OS-level configuration
           via cloud-init and Ansible.}
  \label{fig:prov-os}
\end{figure}

Figure~\ref{fig:prov-cluster} shows the cluster deployment and
recovery flow, orchestrated by Ansible calling the Ambari REST
API (\texttt{cluster\_deploy.yml} and \texttt{deploy\_tasks.yml}).
The diagram covers the happy-path deployment (VDF registration,
blueprint upload, cluster creation, and progress polling) and the
automated recovery logic supporting up to three sequential retry
attempts, each preceded by HDFS corrections and Hive database
reconfiguration, with a final fallback to manual service
initialisation (\texttt{manual\_service\_init.sh}).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{content/assets/provisioning-03-cluster.png}
  \caption{Provisioning workflow — View 3: Ambari REST API cluster
           deployment with automated recovery logic.}
  \label{fig:prov-cluster}
\end{figure}

The resulting architecture from the Default installation profile defines the core capabilities of the Data Lake, separating concerns into functional layers as illustrated in Figure~\ref{fig:arch-default}. This architecture establishes a robust foundation for high-volume data ingestion, distributed storage, and scalable processing engine orchestration, all underpinned by centralized governance and security protocols.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{content/assets/datalake-architecture.png}
  \caption{Final Data Lake Architecture — Functional layers for the Default Profile.}
  \label{fig:arch-default}
\end{figure}

\subsection{Data Lake Installation Profiles}
The cluster provisioning process supports different deployment profiles customised for specific workloads. Based on the selected profile, Ambari will distribute the adequate components across the cluster nodes. The tables below detail the components present and the host responsibilities for each profile.

\begin{table}[htbp]
  \centering
  \caption{Component Mapping for the Default Profile}
  \label{tab:comp-default}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{@{} p{0.38\linewidth} p{0.58\linewidth} @{}}
    \hline
    \textbf{Node \& Role} & \textbf{Installed Components} \\
    \hline
    \textbf{master.cdp (10.0.0.2)} \newline \textit{Master Node (Cluster Management, Core Masters)} & Ambari Server, HBase Master, HistoryServer, NameNode, Phoenix Query Server, Ranger Admin, Ranger UserSync, ResourceManager, Secondary NameNode, Spark 3 JobHistoryServer, ZooKeeper Server \\
    \hline
    \textbf{node1.cdp (10.0.0.3)} \newline \textit{Worker Node 1 (Data/Compute, specialized services)} & DataNode, Hive Metastore, Hive Server, Kafka Broker, NFS Gateway, NiFi CA, NiFi Master, NodeManager, ZooKeeper Server \\
    \hline
    \textbf{node2.cdp (10.0.0.4)} \newline \textit{Worker Node 2 (Data/Compute, specialized services)} & DataNode, Kafka Broker, NodeManager, Spark 3 ThriftServer, ZooKeeper Server \\
    \hline
    \textbf{node3.cdp (10.0.0.5)} \newline \textit{Worker Node 3 (Data/Compute, specialized services)} & DataNode, HBase RegionServer, Kafka Broker, NodeManager, Ranger TagSync, Spark 3 Livy2 Server \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Component Mapping for the Data Science Profile}
  \label{tab:comp-data-science}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{@{} p{0.38\linewidth} p{0.58\linewidth} @{}}
    \hline
    \textbf{Node \& Role} & \textbf{Installed Components} \\
    \hline
    \textbf{master.cdp (10.0.0.2)} \newline \textit{Master Node (Cluster Management, Core Masters)} & HistoryServer, NameNode, Ranger Admin, Ranger UserSync, ResourceManager, Secondary NameNode, Spark 3 JobHistoryServer, ZooKeeper Server \\
    \hline
    \textbf{node1.cdp (10.0.0.3)} \newline \textit{Worker Node 1 (Data/Compute, specialized services)} & DataNode, Hive Metastore, Hive Server, Kafka Broker, NFS Gateway, NodeManager, ZooKeeper Server \\
    \hline
    \textbf{node2.cdp (10.0.0.4)} \newline \textit{Worker Node 2 (Data/Compute, specialized services)} & DataNode, Kafka Broker, NodeManager, Spark 3 ThriftServer, ZooKeeper Server \\
    \hline
    \textbf{node3.cdp (10.0.0.5)} \newline \textit{Worker Node 3 (Data/Compute, specialized services)} & DataNode, Kafka Broker, NodeManager, Ranger TagSync, Spark 3 Livy2 Server \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Component Mapping for the Software Engineering Profile}
  \label{tab:comp-software-engineering}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{@{} p{0.38\linewidth} p{0.58\linewidth} @{}}
    \hline
    \textbf{Node \& Role} & \textbf{Installed Components} \\
    \hline
    \textbf{master.cdp (10.0.0.2)} \newline \textit{Master Node (Cluster Management, Core Masters)} & HBase Master, HistoryServer, NameNode, Phoenix Query Server, ResourceManager, Secondary NameNode, ZooKeeper Server \\
    \hline
    \textbf{node1.cdp (10.0.0.3)} \newline \textit{Worker Node 1 (Data/Compute, specialized services)} & DataNode, NFS Gateway, NiFi CA, NodeManager, ZooKeeper Server \\
    \hline
    \textbf{node2.cdp (10.0.0.4)} \newline \textit{Worker Node 2 (Data/Compute, specialized services)} & DataNode, NiFi Master, NodeManager, ZooKeeper Server \\
    \hline
    \textbf{node3.cdp (10.0.0.5)} \newline \textit{Worker Node 3 (Data/Compute, specialized services)} & DataNode, HBase RegionServer, Kafka Broker, NodeManager \\
    \hline
  \end{tabular}
\end{table}

